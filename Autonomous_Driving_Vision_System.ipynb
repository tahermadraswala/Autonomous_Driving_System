{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahermadraswala/Autonomous-Driving-Vision-System/blob/main/Autonomous_Driving_Vision_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4NLscOI6x6f",
        "outputId": "eb5f688c-7f30-4eb7-f8a8-96f59d25673f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.233)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2ZmDrwekGxNk",
        "outputId": "d2099539-cc27-4341-e49a-dad2ec95244c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n!pip install transformers\\n!pip install gradio\\n!pip install opencv-python\\n!pip install albumentations\\n!pip install segmentation-models-pytorch\\n!pip install timm\\n!pip install scikit-learn\\n!pip install pillow\\n!pip install numpy\\n!pip install matplotlib\\n\\n# For Kaggle dataset download\\n!pip install kaggle\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers\n",
        "!pip install gradio\n",
        "!pip install opencv-python\n",
        "!pip install albumentations\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install timm\n",
        "!pip install scikit-learn\n",
        "!pip install pillow\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "\n",
        "# For Kaggle dataset download\n",
        "!pip install kaggle\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmzeuokS6yWy",
        "outputId": "cd91f9a9-22f2-48e9-861f-2ac7b749cc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ ACCELERATION STATUS: Running on cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from torchvision import transforms, models\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸš€ ACCELERATION STATUS: Running on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s64Zl9aMDhaB"
      },
      "outputs": [],
      "source": [
        "class SemanticSegmentationModel:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.colors = np.array([\n",
        "            [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156],\n",
        "            [190, 153, 153], [153, 153, 153], [250, 170, 30], [220, 220, 0],\n",
        "            [107, 142, 35], [152, 251, 152], [70, 130, 180], [220, 20, 60],\n",
        "            [255, 0, 0], [0, 0, 142], [0, 0, 70], [0, 60, 100],\n",
        "            [0, 80, 100], [0, 0, 230], [119, 11, 32]\n",
        "        ])\n",
        "\n",
        "        self.preprocess = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def segment(self, image: np.ndarray) -> tuple:\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if w > 1024:\n",
        "            scale = 1024 / w\n",
        "            new_w, new_h = 1024, int(h * scale)\n",
        "            proc_img = cv2.resize(image, (new_w, new_h))\n",
        "        else:\n",
        "            proc_img = image\n",
        "\n",
        "        img_rgb = cv2.cvtColor(proc_img, cv2.COLOR_BGR2RGB)\n",
        "        img_pil = Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "        input_tensor = self.preprocess(img_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_tensor)['out'][0]\n",
        "\n",
        "\n",
        "        seg_mask = output.argmax(0).byte().cpu().numpy()\n",
        "\n",
        "\n",
        "        mask_h, mask_w = seg_mask.shape\n",
        "        colored_mask = np.zeros((mask_h, mask_w, 3), dtype=np.uint8)\n",
        "        for class_id in range(min(len(self.colors), seg_mask.max() + 1)):\n",
        "            colored_mask[seg_mask == class_id] = self.colors[class_id]\n",
        "\n",
        "\n",
        "        colored_mask = cv2.resize(colored_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        blended = cv2.addWeighted(image, 0.6, colored_mask, 0.4, 0)\n",
        "        return blended, {\"Info\": \"Segmentation Complete\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7DdtnbQDsYx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ObjectDetectionModel:\n",
        "    \"\"\"\n",
        "    2D Object Detection for dynamic objects using YOLOv8\n",
        "    Optimized for GPU and formatted for Gradio compatibility\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            from ultralytics import YOLO\n",
        "\n",
        "            self.model = YOLO('yolov8n.pt')\n",
        "        except ImportError:\n",
        "            print(\"Error: Ultralytics not installed. Run '!pip install ultralytics'\")\n",
        "            self.model = None\n",
        "\n",
        "\n",
        "        self.vehicle_classes = ['car', 'truck', 'bus', 'train']\n",
        "        self.vru_classes = ['person', 'bicycle', 'motorcycle']\n",
        "        self.traffic_classes = ['traffic light', 'stop sign']\n",
        "\n",
        "    def detect(self, image: np.ndarray) -> tuple:\n",
        "        \"\"\"Perform object detection\"\"\"\n",
        "        if self.model is None:\n",
        "            return image, []\n",
        "\n",
        "\n",
        "        results = self.model(image, conf=0.25, iou=0.45, verbose=False)\n",
        "\n",
        "        detections = []\n",
        "        annotated_image = image.copy()\n",
        "\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            for box in boxes:\n",
        "                # Get coordinates\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                confidence = float(box.conf[0])\n",
        "                class_id = int(box.cls[0])\n",
        "                class_name = self.model.names[class_id]\n",
        "\n",
        "                # --- PRIORITY & COLOR LOGIC (Restored) ---\n",
        "                if class_name in self.vru_classes:\n",
        "                    color = (0, 0, 255)      # Red for pedestrians/cyclists\n",
        "                    priority = \"High\"\n",
        "                elif class_name in self.traffic_classes:\n",
        "                    color = (255, 165, 0)    # Orange for traffic signs\n",
        "                    priority = \"High\"\n",
        "                elif class_name in self.vehicle_classes:\n",
        "                    color = (0, 255, 0)      # Green for vehicles\n",
        "                    priority = \"Medium\"\n",
        "                else:\n",
        "                    color = (255, 255, 0)    # Yellow for others\n",
        "                    priority = \"Low\"\n",
        "\n",
        "\n",
        "                cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "\n",
        "                label = f\"{class_name} {confidence:.2f}\"\n",
        "                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "                cv2.rectangle(annotated_image, (x1, y1 - label_size[1] - 10),\n",
        "                            (x1 + label_size[0], y1), color, -1)\n",
        "                cv2.putText(annotated_image, label, (x1, y1 - 5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "\n",
        "                detections.append({\n",
        "                    'class': class_name,\n",
        "                    'confidence': f\"{confidence:.2f}\",\n",
        "                    'bbox': f\"({x1},{y1},{x2},{y2})\",\n",
        "                    'priority': priority\n",
        "                })\n",
        "\n",
        "        return annotated_image, detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-QWyyZwDv_b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "class LaneDetectionModel:\n",
        "    \"\"\"\n",
        "    Lane Line Detection for lateral control\n",
        "    Uses Hough Transform and curve fitting\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.roi_vertices = None\n",
        "\n",
        "    def detect_lanes(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
        "        \"\"\"Detect lane lines in the image\"\"\"\n",
        "\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "\n",
        "        edges = cv2.Canny(blur, 50, 150)\n",
        "\n",
        "\n",
        "        height, width = image.shape[:2]\n",
        "        roi_vertices = np.array([[\n",
        "            (width * 0.1, height),\n",
        "            (width * 0.45, height * 0.6),\n",
        "            (width * 0.55, height * 0.6),\n",
        "            (width * 0.9, height)\n",
        "        ]], dtype=np.int32)\n",
        "\n",
        "\n",
        "        mask = np.zeros_like(edges)\n",
        "        cv2.fillPoly(mask, roi_vertices, 255)\n",
        "        masked_edges = cv2.bitwise_and(edges, mask)\n",
        "\n",
        "\n",
        "        lines = cv2.HoughLinesP(masked_edges, rho=2, theta=np.pi/180,\n",
        "                               threshold=50, minLineLength=40, maxLineGap=100)\n",
        "\n",
        "\n",
        "        lane_image = image.copy()\n",
        "\n",
        "        if lines is not None:\n",
        "\n",
        "            left_lines = []\n",
        "            right_lines = []\n",
        "\n",
        "            for line in lines:\n",
        "                x1, y1, x2, y2 = line[0]\n",
        "                if x2 - x1 == 0:\n",
        "                    continue\n",
        "                slope = (y2 - y1) / (x2 - x1)\n",
        "\n",
        "                if abs(slope) < 0.5:\n",
        "                    continue\n",
        "\n",
        "                if slope < 0:\n",
        "                    left_lines.append(line[0])\n",
        "                else:\n",
        "                    right_lines.append(line[0])\n",
        "\n",
        "\n",
        "            def draw_lane_line(lines, color):\n",
        "                if len(lines) > 0:\n",
        "\n",
        "                    lines_array = np.array(lines)\n",
        "                    x_coords = np.concatenate([lines_array[:, [0, 2]]])\n",
        "                    y_coords = np.concatenate([lines_array[:, [1, 3]]])\n",
        "\n",
        "\n",
        "                    if len(x_coords) > 0:\n",
        "                        poly = np.polyfit(y_coords.flatten(), x_coords.flatten(), deg=1)\n",
        "                        y1, y2 = height, int(height * 0.6)\n",
        "                        x1, x2 = int(np.polyval(poly, y1)), int(np.polyval(poly, y2))\n",
        "                        cv2.line(lane_image, (x1, y1), (x2, y2), color, 10)\n",
        "\n",
        "            draw_lane_line(left_lines, (255, 0, 0))\n",
        "            draw_lane_line(right_lines, (0, 255, 0))\n",
        "\n",
        "\n",
        "            if len(left_lines) > 0 and len(right_lines) > 0:\n",
        "                left_x = np.mean([line[0] for line in left_lines])\n",
        "                right_x = np.mean([line[0] for line in right_lines])\n",
        "                lane_center = (left_x + right_x) / 2\n",
        "                image_center = width / 2\n",
        "                deviation = ((lane_center - image_center) / width) * 100\n",
        "\n",
        "                info = {\n",
        "                    'Left Lane': 'Detected',\n",
        "                    'Right Lane': 'Detected',\n",
        "                    'Deviation': f\"{deviation:.2f}% {'right' if deviation > 0 else 'left'}\"\n",
        "                }\n",
        "            else:\n",
        "                info = {\n",
        "                    'Left Lane': 'Detected' if len(left_lines) > 0 else 'Not Detected',\n",
        "                    'Right Lane': 'Detected' if len(right_lines) > 0 else 'Not Detected',\n",
        "                    'Deviation': 'N/A'\n",
        "                }\n",
        "        else:\n",
        "            info = {\n",
        "                'Left Lane': 'Not Detected',\n",
        "                'Right Lane': 'Not Detected',\n",
        "                'Deviation': 'N/A'\n",
        "            }\n",
        "\n",
        "\n",
        "        cv2.polylines(lane_image, roi_vertices, True, (0, 255, 255), 2)\n",
        "\n",
        "        return lane_image, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzkU4Ml5DzxL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "class TrafficSignRecognition:\n",
        "    \"\"\"\n",
        "    Traffic Sign and Light Recognition\n",
        "    Recognizes: Stop, Yield, Speed Limit, Traffic Light States\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "\n",
        "        self.traffic_light_colors = {\n",
        "            'red': ([0, 0, 100], [80, 80, 255]),\n",
        "            'yellow': ([0, 100, 100], [80, 255, 255]),\n",
        "            'green': ([0, 100, 0], [80, 255, 80])\n",
        "        }\n",
        "\n",
        "    def detect_traffic_lights(self, image: np.ndarray) -> Tuple[np.ndarray, List[Dict]]:\n",
        "        \"\"\"Detect traffic lights and their states\"\"\"\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        result_image = image.copy()\n",
        "        detections = []\n",
        "\n",
        "        for color_name, (lower, upper) in self.traffic_light_colors.items():\n",
        "\n",
        "            lower_bound = np.array(lower)\n",
        "            upper_bound = np.array(upper)\n",
        "            mask = cv2.inRange(image, lower_bound, upper_bound)\n",
        "\n",
        "\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
        "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if area > 100:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    aspect_ratio = h / w if w > 0 else 0\n",
        "\n",
        "\n",
        "                    if 0.8 < aspect_ratio < 3.0:\n",
        "                        cv2.rectangle(result_image, (x, y), (x+w, y+h),\n",
        "                                    (0, 255, 255), 2)\n",
        "                        cv2.putText(result_image, f\"{color_name.upper()} LIGHT\",\n",
        "                                  (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                                  0.5, (0, 255, 255), 2)\n",
        "\n",
        "                        detections.append({\n",
        "                            'type': 'Traffic Light',\n",
        "                            'state': color_name.upper(),\n",
        "                            'action': self.get_action(color_name),\n",
        "                            'location': f\"({x},{y})\"\n",
        "                        })\n",
        "\n",
        "        return result_image, detections\n",
        "\n",
        "    def get_action(self, light_color: str) -> str:\n",
        "        \"\"\"Get recommended action based on traffic light\"\"\"\n",
        "        actions = {\n",
        "            'red': 'STOP',\n",
        "            'yellow': 'PREPARE TO STOP',\n",
        "            'green': 'GO'\n",
        "        }\n",
        "        return actions.get(light_color, 'UNKNOWN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pRYFqZyD26B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DepthEstimationModel:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "        midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
        "        self.transform = midas_transforms.small_transform\n",
        "\n",
        "    def estimate_depth(self, image: np.ndarray) -> tuple:\n",
        "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "        input_batch = self.transform(img_rgb).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = self.model(input_batch)\n",
        "            prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=img_rgb.shape[:2],\n",
        "                mode='bicubic',\n",
        "                align_corners=False\n",
        "            ).squeeze()\n",
        "\n",
        "\n",
        "        depth_map = prediction.cpu().numpy()\n",
        "\n",
        "        depth_norm = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "        depth_colored = cv2.applyColorMap(depth_norm, cv2.COLORMAP_MAGMA)\n",
        "\n",
        "        return depth_colored, {'Status': 'Depth Calculated'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkbMqcbJD64D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class AutonomousDrivingSystem:\n",
        "    \"\"\"\n",
        "    Complete Autonomous Driving System\n",
        "    Integrates all CV components for comprehensive scene understanding\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Autonomous Driving System...\")\n",
        "        self.segmentation = SemanticSegmentationModel()\n",
        "        print(\"âœ“ Semantic Segmentation Model Loaded\")\n",
        "\n",
        "        self.object_detection = ObjectDetectionModel()\n",
        "        print(\"âœ“ Object Detection Model Loaded\")\n",
        "\n",
        "        self.lane_detection = LaneDetectionModel()\n",
        "        print(\"âœ“ Lane Detection Model Loaded\")\n",
        "\n",
        "        self.traffic_recognition = TrafficSignRecognition()\n",
        "        print(\"âœ“ Traffic Sign Recognition Loaded\")\n",
        "\n",
        "        self.depth_estimation = DepthEstimationModel()\n",
        "        print(\"âœ“ Depth Estimation Model Loaded\")\n",
        "\n",
        "        print(\"System Ready!\")\n",
        "\n",
        "    def process_frame(self, image: np.ndarray, selected_modules: List[str]) -> Dict:\n",
        "        \"\"\"Process frame with selected modules\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        if \"Semantic Segmentation\" in selected_modules:\n",
        "            seg_result, seg_stats = self.segmentation.segment(image)\n",
        "            results['segmentation'] = (seg_result, seg_stats)\n",
        "\n",
        "        if \"Object Detection\" in selected_modules:\n",
        "            det_result, detections = self.object_detection.detect(image)\n",
        "            results['detection'] = (det_result, detections)\n",
        "\n",
        "        if \"Lane Detection\" in selected_modules:\n",
        "            lane_result, lane_info = self.lane_detection.detect_lanes(image)\n",
        "            results['lanes'] = (lane_result, lane_info)\n",
        "\n",
        "        if \"Traffic Sign & Light Recognition\" in selected_modules:\n",
        "            traffic_result, traffic_info = self.traffic_recognition.detect_traffic_lights(image)\n",
        "            results['traffic'] = (traffic_result, traffic_info)\n",
        "\n",
        "        if \"Depth Estimation\" in selected_modules:\n",
        "            depth_result, depth_info = self.depth_estimation.estimate_depth(image)\n",
        "            results['depth'] = (depth_result, depth_info)\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdevIwo4D95g"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create Gradio interface for the system\"\"\"\n",
        "\n",
        "\n",
        "    system = AutonomousDrivingSystem()\n",
        "\n",
        "    def process_image(image, seg_check, det_check, lane_check, traffic_check, depth_check):\n",
        "        \"\"\"Process image with selected modules\"\"\"\n",
        "        if image is None:\n",
        "            return None, None, None, None, None, \"Please upload an image\"\n",
        "\n",
        "\n",
        "        if len(image.shape) == 2:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "        elif image.shape[2] == 4:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)\n",
        "\n",
        "        selected = []\n",
        "        if seg_check: selected.append(\"Semantic Segmentation\")\n",
        "        if det_check: selected.append(\"Object Detection\")\n",
        "        if lane_check: selected.append(\"Lane Detection\")\n",
        "        if traffic_check: selected.append(\"Traffic Sign & Light Recognition\")\n",
        "        if depth_check: selected.append(\"Depth Estimation\")\n",
        "\n",
        "        if not selected:\n",
        "            return None, None, None, None, None, \"Please select at least one module\"\n",
        "\n",
        "        results = system.process_frame(image, selected)\n",
        "\n",
        "\n",
        "        seg_img = results.get('segmentation', (None, {}))[0]\n",
        "        det_img = results.get('detection', (None, []))[0]\n",
        "        lane_img = results.get('lanes', (None, {}))[0]\n",
        "        traffic_img = results.get('traffic', (None, []))[0]\n",
        "        depth_img = results.get('depth', (None, {}))[0]\n",
        "\n",
        "\n",
        "        summary = \"=== AUTONOMOUS DRIVING SYSTEM ANALYSIS ===\\n\\n\"\n",
        "\n",
        "        if 'segmentation' in results:\n",
        "            summary += \"SEMANTIC SEGMENTATION:\\n\"\n",
        "            for key, val in results['segmentation'][1].items():\n",
        "                summary += f\"  â€¢ {key}: {val}\\n\"\n",
        "            summary += \"\\n\"\n",
        "\n",
        "        if 'detection' in results:\n",
        "            summary += \"OBJECT DETECTION:\\n\"\n",
        "            detections = results['detection'][1]\n",
        "            if detections:\n",
        "                for det in detections[:5]:\n",
        "                    summary += f\"  â€¢ {det['class']} (Conf: {det['confidence']}, Priority: {det['priority']})\\n\"\n",
        "                if len(detections) > 5:\n",
        "                    summary += f\"  ... and {len(detections)-5} more objects\\n\"\n",
        "            else:\n",
        "                summary += \"  â€¢ No objects detected\\n\"\n",
        "            summary += \"\\n\"\n",
        "\n",
        "        if 'lanes' in results:\n",
        "            summary += \"LANE DETECTION:\\n\"\n",
        "            for key, val in results['lanes'][1].items():\n",
        "                summary += f\"  â€¢ {key}: {val}\\n\"\n",
        "            summary += \"\\n\"\n",
        "\n",
        "        if 'traffic' in results:\n",
        "            summary += \"TRAFFIC LIGHTS:\\n\"\n",
        "            traffic_info = results['traffic'][1]\n",
        "            if traffic_info:\n",
        "                for info in traffic_info:\n",
        "                    summary += f\"  â€¢ {info['state']} â†’ {info['action']}\\n\"\n",
        "            else:\n",
        "                summary += \"  â€¢ No traffic lights detected\\n\"\n",
        "            summary += \"\\n\"\n",
        "\n",
        "        if 'depth' in results:\n",
        "            summary += \"DEPTH ESTIMATION:\\n\"\n",
        "            for key, val in results['depth'][1].items():\n",
        "                summary += f\"  â€¢ {key}: {val}\\n\"\n",
        "\n",
        "        return seg_img, det_img, lane_img, traffic_img, depth_img, summary\n",
        "\n",
        "\n",
        "    with gr.Blocks(title=\"Autonomous Driving System\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ðŸš— Autonomous Driving System - Computer Vision Project\n",
        "        ### Advanced Multi-Module Vision System for Self-Driving Vehicles\n",
        "\n",
        "        This system implements **5 critical computer vision components** for autonomous driving:\n",
        "        1. **Semantic Segmentation** - Scene understanding (Road, Sky, Vehicle, Pedestrian)\n",
        "        2. **Object Detection** - Dynamic object tracking (Cars, Trucks, Pedestrians, Cyclists)\n",
        "        3. **Lane Detection** - Lane boundary identification for lateral control\n",
        "        4. **Traffic Sign & Light Recognition** - Traffic rule compliance\n",
        "        5. **Monocular Depth Estimation** - Distance estimation for safe velocity control\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                input_image = gr.Image(label=\"Upload Driving Scene Image\", type=\"numpy\")\n",
        "\n",
        "                gr.Markdown(\"### Select Modules to Run:\")\n",
        "                seg_check = gr.Checkbox(label=\"Semantic Segmentation\", value=True)\n",
        "                det_check = gr.Checkbox(label=\"Object Detection\", value=True)\n",
        "                lane_check = gr.Checkbox(label=\"Lane Detection\", value=True)\n",
        "                traffic_check = gr.Checkbox(label=\"Traffic Sign & Light Recognition\", value=True)\n",
        "                depth_check = gr.Checkbox(label=\"Depth Estimation\", value=True)\n",
        "\n",
        "                process_btn = gr.Button(\"ðŸš€ Analyze Scene\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                summary_output = gr.Textbox(label=\"Analysis Summary\", lines=15)\n",
        "\n",
        "        gr.Markdown(\"### Module Outputs:\")\n",
        "\n",
        "        with gr.Row():\n",
        "            seg_output = gr.Image(label=\"Semantic Segmentation\")\n",
        "            det_output = gr.Image(label=\"Object Detection\")\n",
        "\n",
        "        with gr.Row():\n",
        "            lane_output = gr.Image(label=\"Lane Detection\")\n",
        "            traffic_output = gr.Image(label=\"Traffic Recognition\")\n",
        "\n",
        "        depth_output = gr.Image(label=\"Depth Estimation\")\n",
        "\n",
        "        process_btn.click(\n",
        "            fn=process_image,\n",
        "            inputs=[input_image, seg_check, det_check, lane_check, traffic_check, depth_check],\n",
        "            outputs=[seg_output, det_output, lane_output, traffic_output, depth_output, summary_output]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ml-Wps4WECrA",
        "outputId": "a3f8ed87-4c72-40d5-a6a9-f76b840deb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Autonomous Driving System...\n",
            "âœ“ Semantic Segmentation Model Loaded\n",
            "âœ“ Object Detection Model Loaded\n",
            "âœ“ Lane Detection Model Loaded\n",
            "âœ“ Traffic Sign Recognition Loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights:  None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Depth Estimation Model Loaded\n",
            "System Ready!\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://66fc5b3fdbd0690dd8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://66fc5b3fdbd0690dd8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c8964095ac0 [unset]> is bound to a different event loop\n",
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c8964095ac0 [unset]> is bound to a different event loop\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    demo = create_gradio_interface()\n",
        "    demo.launch(share=True, debug=True)\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AUTONOMOUS DRIVING VISION SYSTEM LAUNCHED SUCCESSFULLY!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nTo use in Google Colab:\")\n",
        "    print(\"1. Run all installation commands at the top\")\n",
        "    print(\"2. Run this entire script\")\n",
        "    print(\"3. Upload driving scene images (download from Kaggle datasets)\")\n",
        "    print(\"\\nRecommended Kaggle Datasets:\")\n",
        "    print(\"â€¢ BDD100K: berkeley-deep-drive/bdd100k\")\n",
        "    print(\"â€¢ Cityscapes: dansbecker/cityscapes-image-pairs\")\n",
        "    print(\"â€¢ KITTI: kitti-dataset/kitti\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIl3LyefGeog"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv7tSNfaEDbN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}